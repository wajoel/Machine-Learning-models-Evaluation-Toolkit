# -*- coding: utf-8 -*-
"""MLtoolkit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKvk7kZzZBh5VAysbV3nMHmzcnq1-TuX
"""

import numpy as np
import pandas as pd
from sklearn.metrics import (confusion_matrix, roc_curve, auc, precision_recall_curve,
                             classification_report, mean_squared_error, r2_score)
from scipy.stats import ttest_rel
import matplotlib.pyplot as plt
import seaborn as sns

class ModelEvaluationToolkit:
    def __init__(self):
        self.results = {}

    def evaluate_classification(self, y_true, y_pred, y_probs=None):
        """Evaluate classification model performance."""
        print("\n--- Classification Report ---")
        report = classification_report(y_true, y_pred)
        print(report)

        cm = confusion_matrix(y_true, y_pred)
        print("\n--- Confusion Matrix ---")
        print(cm)

        plt.figure(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["0", "1"], yticklabels=["0", "1"])
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.title("Confusion Matrix")
        plt.show()

        if y_probs is not None:
            fpr, tpr, _ = roc_curve(y_true, y_probs)
            roc_auc = auc(fpr, tpr)
            print(f"ROC AUC: {roc_auc:.4f}")

            plt.figure()
            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('Receiver Operating Characteristic')
            plt.legend(loc="lower right")
            plt.show()

            precision, recall, _ = precision_recall_curve(y_true, y_probs)
            plt.figure()
            plt.plot(recall, precision, color='blue', lw=2)
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title('Precision-Recall Curve')
            plt.show()

    def evaluate_regression(self, y_true, y_pred):
        """Evaluate regression model performance."""
        mse = mean_squared_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        print("\n--- Regression Metrics ---")
        print(f"Mean Squared Error: {mse:.4f}")
        print(f"R-squared: {r2:.4f}")

        plt.figure(figsize=(6, 6))
        plt.scatter(y_true, y_pred, alpha=0.6)
        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2)
        plt.xlabel("True Values")
        plt.ylabel("Predicted Values")
        plt.title("True vs Predicted Values")
        plt.show()

    def compare_models(self, scores_model1, scores_model2):
        """Compare two models using paired t-tests."""
        print("\n--- Model Comparison ---")
        t_stat, p_value = ttest_rel(scores_model1, scores_model2)
        print(f"T-statistic: {t_stat:.4f}")
        print(f"P-value: {p_value:.4f}")

        if p_value < 0.05:
            print("The performance difference between the two models is statistically significant.")
        else:
            print("No statistically significant performance difference between the models.")

if __name__ == "__main__":
    print("Welcome to the Model Evaluation Toolkit!")

    # Example usage
    toolkit = ModelEvaluationToolkit()

    print("\n--- Example Classification Evaluation ---")
    y_true_class = [0, 1, 0, 1, 1, 0, 1, 0]
    y_pred_class = [0, 1, 0, 1, 1, 0, 0, 1]
    y_probs_class = [0.1, 0.8, 0.3, 0.9, 0.85, 0.2, 0.4, 0.7]
    toolkit.evaluate_classification(y_true_class, y_pred_class, y_probs_class)

    print("\n--- Example Regression Evaluation ---")
    y_true_reg = [3.5, 4.2, 5.1, 6.8]
    y_pred_reg = [3.7, 4.0, 5.2, 6.6]
    toolkit.evaluate_regression(y_true_reg, y_pred_reg)

    print("\n--- Example Model Comparison ---")
    model1_scores = [0.85, 0.88, 0.89, 0.91, 0.87]
    model2_scores = [0.82, 0.84, 0.86, 0.89, 0.85]
    toolkit.compare_models(model1_scores, model2_scores)